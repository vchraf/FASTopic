{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b2bda3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "import networkx as nx\n",
    "import jellyfish\n",
    "from segtok.segmenter import split_multi\n",
    "from segtok.tokenizer import web_tokenizer, split_contractions\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from fasttext import load_model\n",
    "fasttext_model = 'wiki/wiki.fr.bin'\n",
    "fmodel = load_model(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a3b349cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtPreprocessing(txt):\n",
    "    x = str(txt).lower()\n",
    "    x = re.sub(r'http\\S+', 'URL', x)\n",
    "    x = re.sub(r'@[^\\s]+', 'USER', x)\n",
    "    x = re.sub(r'#[^\\s]+', 'HASHTAG',x)\n",
    "    x = re.sub('<.*?>', ' ', x)\n",
    "    x = re.sub(' +', ' ', x)\n",
    "    x = re.sub(\"\\[.*?\\]\",\" \",x)\n",
    "    x = re.sub(\"[()!?',:;!.\\\"\\\\n]\",\" \", x)\n",
    "    x = re.sub(\"['\\\"\\\\n]\",\" \", x)\n",
    "    x = re.sub(\"www.\\S+\",\" \", x)\n",
    "    x = re.sub('[{!\"#$%&\\'()*’+,-./:;<=>?@[\\\\]^_`{»|«}~}]', ' ',x)\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    x = re.sub(' +', ' ',x)\n",
    "    x = x.strip()\n",
    "#     x = \" \".join([word for word in x.split(\" \") if word not in stopWords])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f3756aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "data = pd.read_parquet('/data/big-data-collection/OTHERS/data_esg.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a782886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['TITRE','CONTENU']]\n",
    "data[\"txt\"] = data['TITRE'] +\" \"+data['CONTENU']\n",
    "data[\"txtClean\"] = data.txt.apply(lambda x:txtPreprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b682bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/notebooks/others/french_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d24847",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"txtClean\"] = data.text.apply(lambda x:txtPreprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txtVec = []\n",
    "# for txt in _data.txtClean.to_list():\n",
    "#     txtVec.append(fmodel.get_sentence_vector(txt))\n",
    "# _data[\"txtVec\"] = txtVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "259d4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = open('./stopwordFr', 'r').readlines()\n",
    "for i in range(len(stopWords)):\n",
    "    stopWords[i] = stopWords[i].replace('\\n', '').replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b60ac492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopWords = list(set(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df293760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# umap_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c7ae2cfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getClusers(embeddings):\n",
    "    fifteen = len(embeddings)*.22\n",
    "    min_cluster_size, min_samples = 20, 2\n",
    "    clusters = np.full(embeddings.shape[0], -1)\n",
    "    _clusters = np.arange(len(clusters))\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', low_memory=False, verbose=False)\n",
    "    if len(embeddings)//50000 < 2:\n",
    "        _umap_embeddings = umap_model.fit_transform(embeddings)\n",
    "        _embeddings = np.nan_to_num(_umap_embeddings) \n",
    "    else:\n",
    "        umap_model.fit(embeddings[:int(fifteen)])\n",
    "        chunks = np.array_split(embeddings, len(embeddings)//50000)\n",
    "        _umap_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            _umap_embeddings.extend(umap_model.transform(embeddings))\n",
    "        _embeddings = np.nan_to_num(_umap_embeddings)    \n",
    "    \n",
    "    while(len(_clusters)>fifteen):\n",
    "        nbrCl = len(set(clusters))-1\n",
    "        hdbscan_model =hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric='euclidean',\n",
    "                                       cluster_selection_method='eom', prediction_data=True)\n",
    "        hdbscan_model.fit(_embeddings)\n",
    "        indexes = np.argwhere(hdbscan_model.labels_!=-1)\n",
    "        for i, v in zip(indexes, hdbscan_model.labels_[indexes]):\n",
    "            clusters[_clusters[i]] = nbrCl + v\n",
    "        _clusters = np.argwhere(hdbscan_model.labels_==-1)\n",
    "        _embeddings = _embeddings[_clusters]\n",
    "        _embeddings = _embeddings.reshape(len(_clusters),-1)\n",
    "        min_cluster_size, min_samples = 10, 1\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8ec4625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50002//50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a88b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopcKeywords(doc, nbrKeywords=15):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=stopWords)\n",
    "    tfidf = tfidf_vectorizer.fit_transform([doc])\n",
    "    score = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n",
    "    candidates = np.array(tfidf_vectorizer.get_feature_names_out())[score[:277]]\n",
    "    return ' '.join(candidates)\n",
    "#     doc_embedding = fmodel.get_sentence_vector(doc)\n",
    "#     candidate_embeddings = []\n",
    "#     for w in candidates:\n",
    "#         candidate_embeddings.append(fmodel.get_sentence_vector(w))\n",
    "#     distances = cosine_similarity([doc_embedding], candidate_embeddings)[0]\n",
    "#     return ' '.join([ candidates[index] for index in distances.argsort()[-nbrKeywords:]][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24e07c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f178c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b2d1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORD_WEIGHT = 'bi'\n",
    "class DataCore(object):\n",
    "    \n",
    "    def __init__(self, text, stopword_set, windowsSize, n, tagsToDiscard = set(['u', 'd']), exclude = set(string.punctuation)):\n",
    "        self.number_of_sentences = 0\n",
    "        self.number_of_words = 0\n",
    "        self.terms = {}\n",
    "        self.candidates = {}\n",
    "        self.sentences_obj = []\n",
    "        self.sentences_str = []\n",
    "        self.G = nx.DiGraph()\n",
    "        self.exclude = exclude\n",
    "        self.tagsToDiscard = tagsToDiscard\n",
    "        self.freq_ns = {}\n",
    "        for i in range(n):\n",
    "            self.freq_ns[i+1] = 0.\n",
    "        self.stopword_set = stopword_set\n",
    "        self._build(text, windowsSize, n)\n",
    "\n",
    "    def build_candidate(self, candidate_string):\n",
    "        sentences_str = [w for w in split_contractions(web_tokenizer(candidate_string.lower())) if not (w.startswith(\"'\") and len(w) > 1) and len(w) > 0]\n",
    "        candidate_terms = []\n",
    "        for (i, word) in enumerate(sentences_str):\n",
    "            tag = self.getTag(word, i)\n",
    "            term_obj = self.getTerm(word, save_non_seen=False)\n",
    "            if term_obj.tf == 0:\n",
    "                term_obj = None\n",
    "            candidate_terms.append( (tag, word, term_obj) )\n",
    "        if len([cand for cand in candidate_terms if cand[2] != None]) == 0:\n",
    "            invalid_virtual_cand = composed_word(None)\n",
    "            return invalid_virtual_cand\n",
    "        virtual_cand = composed_word(candidate_terms)\n",
    "        return virtual_cand\n",
    "\n",
    "    # Build the datacore features\n",
    "    def _build(self, text, windowsSize, n):\n",
    "        text = self.pre_filter(text)\n",
    "        self.sentences_str = [ [w for w in split_contractions(web_tokenizer(s)) if not (w.startswith(\"'\") and len(w) > 1) and len(w) > 0] for s in list(split_multi(text)) if len(s.strip()) > 0]\n",
    "        self.number_of_sentences = len(self.sentences_str)\n",
    "        pos_text = 0\n",
    "        block_of_word_obj = []\n",
    "        sentence_obj_aux = []\n",
    "        for (sentence_id, sentence) in enumerate(self.sentences_str):\n",
    "            sentence_obj_aux = []\n",
    "            block_of_word_obj = []\n",
    "            for (pos_sent, word) in enumerate(sentence):\n",
    "                if len([c for c in word if c in self.exclude]) == len(word): # If the word is based on exclude chars\n",
    "                    if len(block_of_word_obj) > 0:\n",
    "                        sentence_obj_aux.append( block_of_word_obj )\n",
    "                        block_of_word_obj = []\n",
    "                else:\n",
    "                    tag = self.getTag(word, pos_sent)\n",
    "                    term_obj = self.getTerm(word)\n",
    "                    term_obj.addOccur(tag, sentence_id, pos_sent, pos_text)\n",
    "                    pos_text += 1\n",
    "\n",
    "                    #Create co-occurrence matrix\n",
    "                    if tag not in self.tagsToDiscard:\n",
    "                        word_windows = list(range( max(0, len(block_of_word_obj)-windowsSize), len(block_of_word_obj) ))\n",
    "                        for w in word_windows:\n",
    "                            if block_of_word_obj[w][0] not in self.tagsToDiscard: \n",
    "                                self.addCooccur(block_of_word_obj[w][2], term_obj)\n",
    "                    #Generate candidate keyphrase list\n",
    "                    candidate = [ (tag, word, term_obj) ]\n",
    "                    cand = composed_word(candidate)\n",
    "                    self.addOrUpdateComposedWord(cand)\n",
    "                    word_windows = list(range( max(0, len(block_of_word_obj)-(n-1)), len(block_of_word_obj) ))[::-1]\n",
    "                    for w in word_windows:\n",
    "                        candidate.append(block_of_word_obj[w])\n",
    "                        self.freq_ns[len(candidate)] += 1.\n",
    "                        cand = composed_word(candidate[::-1])\n",
    "                        self.addOrUpdateComposedWord(cand)\n",
    "\n",
    "                    # Add term to the block of words' buffer\n",
    "                    block_of_word_obj.append( (tag, word, term_obj) )\n",
    "\n",
    "            if len(block_of_word_obj) > 0:\n",
    "                sentence_obj_aux.append( block_of_word_obj )\n",
    "\n",
    "            if len(sentence_obj_aux) > 0:\n",
    "                self.sentences_obj.append(sentence_obj_aux)\n",
    "\n",
    "        if len(block_of_word_obj) > 0:\n",
    "            sentence_obj_aux.append( block_of_word_obj )\n",
    "\n",
    "        if len(sentence_obj_aux) > 0:\n",
    "            self.sentences_obj.append(sentence_obj_aux)\n",
    "\n",
    "        self.number_of_words = pos_text\n",
    "\n",
    "    def build_single_terms_features(self, features=None):\n",
    "        validTerms = [ term for term in self.terms.values() if not term.stopword ]\n",
    "        validTFs = (np.array([ x.tf for x in validTerms ]))\n",
    "\n",
    "        if len(validTFs) == 0:\n",
    "            return\n",
    "\n",
    "        avgTF = validTFs.mean()\n",
    "        stdTF = validTFs.std()\n",
    "        maxTF = max([ x.tf for x in self.terms.values()])\n",
    "        list(map(lambda x: x.updateH(maxTF=maxTF, avgTF=avgTF, stdTF=stdTF, number_of_sentences=self.number_of_sentences, features=features), self.terms.values()))\n",
    "\n",
    "    def build_mult_terms_features(self, features=None):\n",
    "        list(map(lambda x: x.updateH(features=features), [cand for cand in self.candidates.values() if cand.isValid()]))\n",
    "\n",
    "    def pre_filter(self, text):\n",
    "        prog = re.compile(\"^(\\\\s*([A-Z]))\")\n",
    "        parts = text.split('\\n')\n",
    "        buffer = ''\n",
    "        for part in parts:\n",
    "            sep = ' '\n",
    "            if prog.match(part):\n",
    "                sep = '\\n\\n'\n",
    "            buffer += sep + part.replace('\\t',' ')\n",
    "        return buffer\n",
    "\n",
    "    def getTag(self, word, i):\n",
    "        try:\n",
    "            w2 = word.replace(\",\",\"\")\n",
    "            float(w2)\n",
    "            return \"d\"\n",
    "        except:\n",
    "            cdigit = len([c for c in word if c.isdigit()])\n",
    "            calpha = len([c for c in word if c.isalpha()])\n",
    "            if ( cdigit > 0 and calpha > 0 ) or (cdigit == 0 and calpha == 0) or len([c for c in word if c in self.exclude]) > 1:\n",
    "                return \"u\"\n",
    "            if len(word) == len([c for c in word if c.isupper()]):\n",
    "                return \"a\"\n",
    "            if len([c for c in word if c.isupper()]) == 1 and len(word) > 1 and word[0].isupper() and i > 0:\n",
    "                return \"n\"\n",
    "        return \"p\"\n",
    "\n",
    "    def getTerm(self, str_word, save_non_seen=True):\n",
    "        unique_term = str_word.lower()\n",
    "        simples_sto = unique_term in self.stopword_set\n",
    "        if unique_term.endswith('s') and len(unique_term) > 3:\n",
    "            unique_term = unique_term[:-1]\n",
    "\n",
    "        if unique_term in self.terms:\n",
    "            return self.terms[unique_term]\n",
    "                \n",
    "        # Include this part\n",
    "        simples_unique_term = unique_term\n",
    "        for pontuation in self.exclude:\n",
    "            simples_unique_term = simples_unique_term.replace(pontuation, '')\n",
    "        # until here\n",
    "        isstopword = simples_sto or unique_term in self.stopword_set or len(simples_unique_term) < 3\n",
    "        \n",
    "        term_id = len(self.terms)\n",
    "        term_obj = single_word(unique_term, term_id, self.G)\n",
    "        term_obj.stopword = isstopword\n",
    "\n",
    "        if save_non_seen:\n",
    "            self.G.add_node(term_id)\n",
    "            self.terms[unique_term] = term_obj\n",
    "\n",
    "        return term_obj\n",
    "\n",
    "    def addCooccur(self, left_term, right_term):\n",
    "        if right_term.id not in self.G[left_term.id]:\n",
    "            self.G.add_edge(left_term.id, right_term.id, TF=0.)\n",
    "        self.G[left_term.id][right_term.id][\"TF\"]+=1.\n",
    "        \n",
    "    def addOrUpdateComposedWord(self, cand):\n",
    "        if cand.unique_kw not in self.candidates:\n",
    "            self.candidates[cand.unique_kw] = cand\n",
    "        else:\n",
    "            self.candidates[cand.unique_kw].uptadeCand(cand)\n",
    "        self.candidates[cand.unique_kw].tf += 1.\n",
    "\n",
    "\n",
    "class composed_word(object):\n",
    "    def __init__(self, terms): # [ (tag, word, term_obj) ]\n",
    "        if terms == None:\n",
    "             self.start_or_end_stopwords = True\n",
    "             self.tags = set()\n",
    "             return\n",
    "        self.tags = set([''.join([ w[0] for w in terms ])])\n",
    "        self.kw = ' '.join( [ w[1] for w in terms ] )\n",
    "        self.unique_kw = self.kw.lower()\n",
    "        self.size = len(terms)\n",
    "        self.terms = [ w[2] for w in terms if w[2] != None ]\n",
    "        self.tf = 0.\n",
    "        self.integrity = 1.\n",
    "        self.H = 1.\n",
    "        self.start_or_end_stopwords = self.terms[0].stopword or self.terms[-1].stopword\n",
    "\n",
    "    def uptadeCand(self, cand):\n",
    "        for tag in cand.tags:\n",
    "            self.tags.add( tag )\n",
    "\n",
    "    def isValid(self):\n",
    "        isValid = False\n",
    "        for tag in self.tags:\n",
    "            isValid = isValid or ( \"u\" not in tag and \"d\" not in tag )\n",
    "        return isValid and not self.start_or_end_stopwords\n",
    "\n",
    "    def get_composed_feature(self, feature_name, discart_stopword=True):\n",
    "        list_of_features = [ getattr(term, feature_name) for term in self.terms if ( discart_stopword and not term.stopword ) or not discart_stopword ]\n",
    "        sum_f  = sum(list_of_features)\n",
    "        prod_f = np.prod(list_of_features)\n",
    "        return ( sum_f, prod_f, prod_f /(sum_f + 1) )\n",
    "\n",
    "    def build_features(self, doc_id=None, keys=None, rel=True, rel_approx=True, isVirtual=False, features=['WFreq', 'WRel', 'tf', 'WCase', 'WPos', 'WSpread'], _stopword=[True, False]):\n",
    "        columns = []\n",
    "        seen = set()\n",
    "        features_cand = []\n",
    "\n",
    "        if doc_id != None:\n",
    "            columns.append('doc_id')\n",
    "            features_cand.append(doc_id)\n",
    "\n",
    "        if keys != None:\n",
    "            if rel:\n",
    "                columns.append('rel')\n",
    "                if self.unique_kw in keys or isVirtual:\n",
    "                    features_cand.append(1)\n",
    "                    seen.add(self.unique_kw)\n",
    "                else:\n",
    "                    features_cand.append(0)\n",
    "\n",
    "            if rel_approx:\n",
    "                columns.append('rel_approx')\n",
    "                max_gold_ = ('', 0.)\n",
    "                for gold_key in keys:\n",
    "                    dist = 1.-jellyfish.levenshtein_distance(gold_key, self.unique_kw ) / max(len(gold_key), len(self.unique_kw)) # _tL\n",
    "                    if max_gold_[1] < dist:\n",
    "                        max_gold_ = ( gold_key, dist )\n",
    "                features_cand.append(max_gold_[1])\n",
    "\n",
    "        columns.append('kw')\n",
    "        features_cand.append(self.unique_kw)\n",
    "        columns.append('h')\n",
    "        features_cand.append(self.H)\n",
    "        columns.append('tf')\n",
    "        features_cand.append(self.tf)\n",
    "        columns.append('size')\n",
    "        features_cand.append(self.size)\n",
    "        columns.append('isVirtual')\n",
    "        features_cand.append(int(isVirtual))\n",
    "\n",
    "        for feature_name in features:\n",
    "\n",
    "            for discart_stopword in _stopword:\n",
    "                (f_sum, f_prod, f_sum_prod) = self.get_composed_feature(feature_name, discart_stopword=discart_stopword)\n",
    "                columns.append('%ss_sum_K%s' % ('n' if discart_stopword else '', feature_name) )\n",
    "                features_cand.append(f_sum)\n",
    "\n",
    "                columns.append('%ss_prod_K%s' % ('n' if discart_stopword else '', feature_name) )\n",
    "                features_cand.append(f_prod)\n",
    "\n",
    "                columns.append('%ss_sum_prod_K%s' % ('n' if discart_stopword else '', feature_name) )\n",
    "                features_cand.append(f_sum_prod)\n",
    "\n",
    "        return (features_cand, columns, seen)\n",
    "\n",
    "    def updateH(self, features=None, isVirtual=False):\n",
    "        sum_H  = 0.\n",
    "        prod_H = 1.\n",
    "\n",
    "        for (t, term_base) in enumerate(self.terms):\n",
    "            if not term_base.stopword:\n",
    "                sum_H += term_base.H\n",
    "                prod_H *= term_base.H\n",
    "\n",
    "            else:\n",
    "                if STOPWORD_WEIGHT == 'bi':\n",
    "                    prob_t1 = 0.\n",
    "                    if term_base.G.has_edge(self.terms[t-1].id, self.terms[ t ].id):\n",
    "                        prob_t1 = term_base.G[self.terms[t-1].id][self.terms[ t ].id][\"TF\"] / self.terms[t-1].tf\n",
    "\n",
    "                    prob_t2 = 0.\n",
    "                    if term_base.G.has_edge(self.terms[ t ].id, self.terms[t+1].id):\n",
    "                        prob_t2 = term_base.G[self.terms[ t ].id][self.terms[t+1].id][\"TF\"] / self.terms[t+1].tf\n",
    "\n",
    "                    prob = prob_t1 * prob_t2\n",
    "                    prod_H *= (1 + (1 - prob ) )\n",
    "                    sum_H -= (1 - prob)\n",
    "                elif STOPWORD_WEIGHT == 'h':\n",
    "                    sum_H += term_base.H\n",
    "                    prod_H *= term_base.H\n",
    "                elif STOPWORD_WEIGHT == 'none':\n",
    "                    pass\n",
    "\n",
    "        tf_used = 1.\n",
    "        if features == None or \"KPF\" in features:\n",
    "            tf_used = self.tf\n",
    "\n",
    "        if isVirtual:\n",
    "            tf_used = np.mean( [term_obj.tf for term_obj in self.terms] )\n",
    "\n",
    "        self.H = prod_H / ( ( sum_H + 1 ) * tf_used )\n",
    "\n",
    "    def updateH_old(self, features=None, isVirtual=False):\n",
    "        sum_H  = 0.\n",
    "        prod_H = 1.\n",
    "\n",
    "        for (t, term_base) in enumerate(self.terms):\n",
    "            if isVirtual and term_base.tf==0:\n",
    "                continue\n",
    "\n",
    "            if term_base.stopword:\n",
    "                prob_t1 = 0.\n",
    "                if term_base.G.has_edge(self.terms[t-1].id, self.terms[ t ].id):\n",
    "                    prob_t1 = term_base.G[self.terms[t-1].id][self.terms[ t ].id][\"TF\"] / self.terms[t-1].tf\n",
    "\n",
    "                prob_t2 = 0.\n",
    "                if term_base.G.has_edge(self.terms[ t ].id, self.terms[t+1].id):\n",
    "                    prob_t2 = term_base.G[self.terms[ t ].id][self.terms[t+1].id][\"TF\"] / self.terms[t+1].tf\n",
    "\n",
    "                prob = prob_t1 * prob_t2\n",
    "                prod_H *= (1 + (1 - prob ) )\n",
    "                sum_H -= (1 - prob)\n",
    "            else:\n",
    "                sum_H += term_base.H\n",
    "                prod_H *= term_base.H\n",
    "        tf_used = 1.\n",
    "        if features == None or \"KPF\" in features:\n",
    "            tf_used = self.tf\n",
    "        if isVirtual:\n",
    "            tf_used = np.mean( [term_obj.tf for term_obj in self.terms] )\n",
    "        self.H = prod_H / ( ( sum_H + 1 ) * tf_used )\n",
    "\n",
    "\n",
    "class single_word(object):\n",
    "\n",
    "    def __init__(self, unique, idx, graph):\n",
    "        self.unique_term = unique\n",
    "        self.id = idx\n",
    "        self.tf = 0.\n",
    "        self.WFreq = 0.0\n",
    "        self.WCase = 0.0\n",
    "        self.tf_a = 0.\n",
    "        self.tf_n = 0.\n",
    "        self.WRel = 1.0\n",
    "        self.PL = 0.\n",
    "        self.PR = 0.\n",
    "        self.occurs = {}\n",
    "        self.WPos = 1.0\n",
    "        self.WSpread = 0.0\n",
    "        self.H = 0.0\n",
    "        self.stopword = False\n",
    "        self.G = graph\n",
    "\n",
    "        self.pagerank = 1.\n",
    "\n",
    "    def updateH(self, maxTF, avgTF, stdTF, number_of_sentences, features=None):\n",
    "        \"\"\"if features == None or \"WRel\" in features:\n",
    "            self.PL = self.WDL / maxTF\n",
    "            self.PR = self.WDR / maxTF\n",
    "            self.WRel = ( (0.5 + (self.PWL * (self.tf / maxTF) + self.PL)) + (0.5 + (self.PWR * (self.tf / maxTF) + self.PR)) )\"\"\"\n",
    "\n",
    "        if features == None or \"WRel\" in features:\n",
    "            self.PL = self.WDL / maxTF\n",
    "            self.PR = self.WDR / maxTF\n",
    "            self.WRel = ( (0.5 + (self.PWL * (self.tf / maxTF))) + (0.5 + (self.PWR * (self.tf / maxTF))) )\n",
    "\n",
    "        if features == None or \"WFreq\" in features:\n",
    "            self.WFreq = self.tf / (avgTF + stdTF)\n",
    "        \n",
    "        if features == None or \"WSpread\" in features:\n",
    "            self.WSpread = len(self.occurs) / number_of_sentences\n",
    "        \n",
    "        if features == None or \"WCase\" in features:\n",
    "            self.WCase = max(self.tf_a, self.tf_n) / (1. + math.log(self.tf))\n",
    "        \n",
    "        if features == None or \"WPos\" in features:\n",
    "            self.WPos = math.log( math.log( 3. + np.median(list(self.occurs.keys())) ) )\n",
    "\n",
    "        self.H = (self.WPos * self.WRel) / (self.WCase + (self.WFreq / self.WRel) + (self.WSpread / self.WRel))\n",
    "        \n",
    "    @property\n",
    "    def WDR(self):\n",
    "        return len( self.G.out_edges(self.id) )\n",
    "\n",
    "    @property\n",
    "    def WIR(self):\n",
    "        return sum( [ d['TF'] for (u,v,d) in self.G.out_edges(self.id, data=True) ] )\n",
    "\n",
    "    @property\n",
    "    def PWR(self):\n",
    "        wir = self.WIR\n",
    "        if wir == 0:\n",
    "            return 0\n",
    "        return self.WDR / wir \n",
    "    \n",
    "    @property\n",
    "    def WDL(self):\n",
    "        return len( self.G.in_edges(self.id) )\n",
    "\n",
    "    @property\n",
    "    def WIL(self):\n",
    "        return sum( [ d['TF'] for (u,v,d) in self.G.in_edges(self.id, data=True) ] )\n",
    "\n",
    "    @property\n",
    "    def PWL(self):\n",
    "        wil = self.WIL\n",
    "        if wil == 0:\n",
    "            return 0\n",
    "        return self.WDL / wil \n",
    "\n",
    "    def addOccur(self, tag, sent_id, pos_sent, pos_text):\n",
    "        if sent_id not in self.occurs:\n",
    "            self.occurs[sent_id] = []\n",
    "\n",
    "        self.occurs[sent_id].append( (pos_sent, pos_text) )\n",
    "        self.tf += 1.\n",
    "\n",
    "        if tag == \"a\":\n",
    "            self.tf_a += 1.\n",
    "        if tag == \"n\":\n",
    "            self.tf_n += 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7688a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Levenshtein(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def __ratio(distance, str_length):\n",
    "        return 1 - float(distance) / float(str_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def ratio(seq1, seq2):\n",
    "        str_distance = Levenshtein.distance(seq1,seq2)\n",
    "        str_length = max(len(seq1),len(seq2))\n",
    "        return Levenshtein.__ratio(str_distance,str_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def distance(seq1, seq2):  \n",
    "        size_x = len(seq1) + 1\n",
    "        size_y = len(seq2) + 1\n",
    "        matrix = np.zeros ((size_x, size_y))\n",
    "        for x in range(size_x):\n",
    "            matrix [x, 0] = x\n",
    "        for y in range(size_y):\n",
    "            matrix [0, y] = y\n",
    "\n",
    "        for x in range(1, size_x):\n",
    "            for y in range(1, size_y):\n",
    "                if seq1[x-1] == seq2[y-1]:\n",
    "                    matrix [x,y] = min(\n",
    "                        matrix[x-1, y] + 1,\n",
    "                        matrix[x-1, y-1],\n",
    "                        matrix[x, y-1] + 1\n",
    "                    )\n",
    "                else:\n",
    "                    matrix [x,y] = min(\n",
    "                        matrix[x-1,y] + 1,\n",
    "                        matrix[x-1,y-1] + 1,\n",
    "                        matrix[x,y-1] + 1\n",
    "                    )\n",
    "        return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca80242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordExtractor(object):\n",
    "\n",
    "    def __init__(self, n=3, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None, stopwords=None):\n",
    "\n",
    "        self.stopword_set = set(stopwords)\n",
    "        self.n = n\n",
    "        self.top = top\n",
    "        self.dedupLim = dedupLim\n",
    "        self.features = features\n",
    "        self.windowsSize = windowsSize\n",
    "        if dedupFunc == 'jaro_winkler' or dedupFunc == 'jaro':\n",
    "            self.dedu_function = self.jaro\n",
    "        elif dedupFunc.lower() == 'sequencematcher' or dedupFunc.lower() == 'seqm':\n",
    "            self.dedu_function = self.seqm\n",
    "        else:\n",
    "            self.dedu_function = self.levs\n",
    "\n",
    "    def jaro(self, cand1, cand2):\n",
    "        return jellyfish.jaro_winkler(cand1, cand2 )\n",
    "\n",
    "    def levs(self, cand1, cand2):\n",
    "        return 1.-jellyfish.levenshtein_distance(cand1, cand2 ) / max(len(cand1),len(cand2))\n",
    "\n",
    "    def seqm(self, cand1, cand2):\n",
    "        return Levenshtein.ratio(cand1, cand2)\n",
    "\n",
    "    def extract_keywords(self, text):\n",
    "        try:\n",
    "            if not(len(text) > 0):\n",
    "                return []\n",
    "\n",
    "            text = text.replace('\\n\\t',' ')\n",
    "            dc = DataCore(text=text, stopword_set=self.stopword_set, windowsSize=self.windowsSize, n=self.n)\n",
    "            dc.build_single_terms_features(features=self.features)\n",
    "            dc.build_mult_terms_features(features=self.features)\n",
    "            resultSet = []\n",
    "            todedup = sorted([cc for cc in dc.candidates.values() if cc.isValid()], key=lambda c: c.H)\n",
    "\n",
    "            if self.dedupLim >= 1.:\n",
    "                return ([ (cand.H, cand.unique_kw) for cand in todedup])[:self.top]\n",
    "\n",
    "            for cand in todedup:\n",
    "                toadd = True\n",
    "                for (h, candResult) in resultSet:\n",
    "                    dist = self.dedu_function(cand.unique_kw, candResult.unique_kw)\n",
    "                    if dist > self.dedupLim:\n",
    "                        toadd = False\n",
    "                        break\n",
    "                if toadd:\n",
    "                    resultSet.append( (cand.H, cand) )\n",
    "                if len(resultSet) == self.top:\n",
    "                    break\n",
    "\n",
    "            return [ cand.kw for (h,cand) in resultSet]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning! Exception: {e} generated by the following text: '{text}' \")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "70426ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(df, txt):\n",
    "    start_time = time.time()\n",
    "    _data = df.copy()\n",
    "    txtVec = []\n",
    "    for txt in _data[txt].to_list():\n",
    "        txtVec.append(fmodel.get_sentence_vector(txt))\n",
    "    _data[\"txtVec\"] = txtVec\n",
    "    \n",
    "    print(\"---txt2Vec: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    embeddings = np.array(_data.txtVec.to_list())\n",
    "    clusters = getClusers(embeddings)\n",
    "    _data['Topic'] = clusters\n",
    "    nbrTopic = len(set(clusters))\n",
    "    _data = _data[_data.Topic != -1]\n",
    "    \n",
    "    documents_per_topic = _data.groupby(['Topic'], as_index=False).agg({'txtClean': ' '.join})\n",
    "    _documents = documents_per_topic.txtClean.values\n",
    "    \n",
    "    print(\"---get initTopic: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    topicKey50 = []\n",
    "    for doc in _documents: topicKey50.append(getTopcKeywords(doc,50))\n",
    "    documents_per_topic['TopicKey50'] = topicKey50\n",
    "    \n",
    "    topVec = []\n",
    "    for txt in documents_per_topic.TopicKey50.to_list():\n",
    "        topVec.append(fmodel.get_sentence_vector(txt))\n",
    "\n",
    "    print(\"---get initTopics Comun word : %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    hdbscan_model =hdbscan.HDBSCAN(metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    hdbscan_model.fit(topVec)\n",
    "\n",
    "    _topics = documents_per_topic.Topic.to_list()\n",
    "    for i, t in enumerate(hdbscan_model.labels_):\n",
    "        if t !=-1: _topics[i] = nbrTopic + t \n",
    "\n",
    "    print(\"---get Topics : %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()  \n",
    "            \n",
    "    documents_per_topic['nTopic'] = _topics\n",
    "    print(documents_per_topic.shape)\n",
    "    documents_per_topic['nTopic'] = documents_per_topic.nTopic.apply(lambda x: np.argwhere(np.array(list(set(_topics))) == x).item())\n",
    "    documents_per_topic = documents_per_topic.groupby(['nTopic'], as_index=False).agg({'txtClean': ' '.join})\n",
    "    _documents = documents_per_topic.txtClean.values\n",
    "    print(documents_per_topic.shape)    \n",
    "    topicKey = []\n",
    "    for doc in _documents:\n",
    "        k_extractor = KeywordExtractor(stopwords=stopWords, n=1, dedupLim=0.9, dedupFunc='jaro', windowsSize=10, top=20, features=None)\n",
    "        topicKey.append(k_extractor.extract_keywords(doc))\n",
    "    documents_per_topic['TopicKey'] = topicKey\n",
    "    \n",
    "    \n",
    "    print(\"---get Topics Keywords: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()      \n",
    "    \n",
    "    nTopic  = {}\n",
    "    for i,j in zip(documents_per_topic.Topic.to_list(), documents_per_topic.nTopic.to_list()): nTopic[i] = j\n",
    "    topic = _data.Topic.apply(lambda x: nTopic[x]).to_list()\n",
    "    \n",
    "    topicDic = {}\n",
    "    cc = Counter(topic)\n",
    "    for i,j in zip(documents_per_topic.nTopic.to_list(),documents_per_topic.TopicKey.to_list()): topicDic[i] = {\"topic\":j,\"f\":cc[i]}\n",
    "    return topic, topicDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "60a65a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---txt2Vec: 98.83606386184692 seconds ---\n",
      "---get initTopic: 12.768213272094727 seconds ---\n",
      "---get initTopics Comun word : 12.644285678863525 seconds ---\n",
      "---get Topics : 0.5437262058258057 seconds ---\n",
      "(794, 4)\n",
      "(460, 2)\n",
      "---get Topics Keywords: 872.4780995845795 seconds ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [147]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time_ \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m td \u001b[38;5;241m=\u001b[39m \u001b[43mgetTopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtxtClean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---ALL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m seconds ---\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time_))\n",
      "Input \u001b[0;32mIn [146]\u001b[0m, in \u001b[0;36mgetTopic\u001b[0;34m(df, txt)\u001b[0m\n\u001b[1;32m     60\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()      \n\u001b[1;32m     62\u001b[0m nTopic  \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mdocuments_per_topic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTopic\u001b[49m\u001b[38;5;241m.\u001b[39mto_list(), documents_per_topic\u001b[38;5;241m.\u001b[39mnTopic\u001b[38;5;241m.\u001b[39mto_list()): nTopic[i] \u001b[38;5;241m=\u001b[39m j\n\u001b[1;32m     64\u001b[0m topic \u001b[38;5;241m=\u001b[39m _data\u001b[38;5;241m.\u001b[39mTopic\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: nTopic[x])\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m     66\u001b[0m topicDic \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Topic'"
     ]
    }
   ],
   "source": [
    "start_time_ = time.time()\n",
    "td = getTopic(data,'txtClean')\n",
    "print(\"---ALL %s seconds ---\" % (time.time() - start_time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e394a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = td.txtClean.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed981ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 19:41:24.602794: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5874295872 exceeds 10% of free system memory.\n",
      "2022-07-11 19:41:25.868126: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5873572080 exceeds 10% of free system memory.\n",
      "2022-07-11 19:41:26.748813: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5874295872 exceeds 10% of free system memory.\n",
      "2022-07-11 19:41:27.770356: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5874295872 exceeds 10% of free system memory.\n",
      "2022-07-11 19:41:35.475174: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 11748591744 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow_hub as hub\n",
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embeddings = embed(txt)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "16f2d1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nTopic</th>\n",
       "      <th>txtClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abdoulaye sow vice président fsf et directeur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a hambourg les sociaux démocrates prêts à cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>niayes moussa baldé souligne les bonds impress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>la demarche du pdidas expliquee par sa coordin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>des experts se penchent sur la gestion durable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>agro industrie un projet de plus d un milliard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>les activites du projet agri jeunes tekki ndaw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>75 nouveaux cas et 3 décès dus au coronavirus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>communiqué du conseil des ministres du 19 févr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>177</td>\n",
       "      <td>pour une meilleure integration de la dimension...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nTopic                                           txtClean\n",
       "0         0  abdoulaye sow vice président fsf et directeur ...\n",
       "1         1  a hambourg les sociaux démocrates prêts à cont...\n",
       "2         2  niayes moussa baldé souligne les bonds impress...\n",
       "3         3  la demarche du pdidas expliquee par sa coordin...\n",
       "4         4  des experts se penchent sur la gestion durable...\n",
       "..      ...                                                ...\n",
       "173     173  agro industrie un projet de plus d un milliard...\n",
       "174     174  les activites du projet agri jeunes tekki ndaw...\n",
       "175     175  75 nouveaux cas et 3 décès dus au coronavirus ...\n",
       "176     176  communiqué du conseil des ministres du 19 févr...\n",
       "177     177  pour une meilleure integration de la dimension...\n",
       "\n",
       "[178 rows x 2 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(td)#[1:2].topic.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464574b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "_data = data.copy()\n",
    "txtVec = []\n",
    "for txt in _data.txtClean.to_list():\n",
    "    txtVec.append(fmodel.get_sentence_vector(txt))\n",
    "_data[\"txtVec\"] = txtVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58719a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(_data.txtVec.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a0e79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458017.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)*.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4fda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63e032e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>UMAP(angular_rp_forest=True, low_memory=False, metric=&#x27;cosine&#x27;, min_dist=0.0, n_components=5, tqdm_kwds={&#x27;bar_format&#x27;: &#x27;{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]&#x27;, &#x27;desc&#x27;: &#x27;Epochs completed&#x27;, &#x27;disable&#x27;: True})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">UMAP</label><div class=\"sk-toggleable__content\"><pre>UMAP(angular_rp_forest=True, low_memory=False, metric=&#x27;cosine&#x27;, min_dist=0.0, n_components=5, tqdm_kwds={&#x27;bar_format&#x27;: &#x27;{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]&#x27;, &#x27;desc&#x27;: &#x27;Epochs completed&#x27;, &#x27;disable&#x27;: True})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "UMAP(angular_rp_forest=True, low_memory=False, metric='cosine', min_dist=0.0, n_components=5, tqdm_kwds={'bar_format': '{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]', 'desc': 'Epochs completed', 'disable': True})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifteen = 100000\n",
    "min_cluster_size, min_samples = 20, 2\n",
    "clusters = np.full(embeddings.shape[0], -1)\n",
    "_clusters = np.arange(len(clusters))\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', low_memory=False, verbose=False)\n",
    "umap_model.fit(embeddings[:int(fifteen)])\n",
    "# _umap_embeddings = umap_model.transform(embeddings)\n",
    "# _embeddings = np.nan_to_num(_umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "614f99c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---txt2Vec: 211.80362462997437 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "___umap_embeddings = umap_model.transform(embeddings[100000:200000])\n",
    "print(\"---txt2Vec: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd53e795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(___umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96e9f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad3a6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = faiss.PCAMatrix (300, 10)\n",
    "mat.train(embeddings[:int(fifteen)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d4a4f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---txt2Vec: 0.0201570987701416 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['journées',\n",
       " 'contribution',\n",
       " 'document',\n",
       " 'agroécologique',\n",
       " 'sénégal',\n",
       " 'transition',\n",
       " 'politique',\n",
       " 'nationale',\n",
       " 'édition',\n",
       " 'consacrée',\n",
       " 'dynamique',\n",
       " 'recommandations',\n",
       " 'plénière',\n",
       " 'tdr',\n",
       " 'janvier',\n",
       " 'grand',\n",
       " 'groupes',\n",
       " 'dakar',\n",
       " 'hautes',\n",
       " 'autorités']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "k_extractor = KeywordExtractor(stopwords=stopWords, n=1, dedupLim=0.9, dedupFunc='jaro', windowsSize=10, top=20, features=None)\n",
    "jj1 = k_extractor.extract_keywords(txt[200])\n",
    "print(\"---txt2Vec: %s seconds ---\" % (time.time() - start_time))\n",
    "jj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6e7d0bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt = data.txtClean.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "085c0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords.append('nbsp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "327f1a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "513579a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---txt2Vec: 0.6962049007415771 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tr = pca.transform(embeddings)\n",
    "print(\"---txt2Vec: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de9addce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "kmeans_clustering() missing 4 required positional arguments: 'n', 'k', 'x', and 'centroids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfaiss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkmeans_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: kmeans_clustering() missing 4 required positional arguments: 'n', 'k', 'x', and 'centroids'"
     ]
    }
   ],
   "source": [
    "faiss.kmeans_clustering(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd8385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ff8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ededd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad923d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29f78c13c374a89b4a66fd67888c243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigscience/bloom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:86\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     83\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Download from hub with caching\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mignore_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflax_model.msgpack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrust_model.ot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/sentence_transformers/util.py:458\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[1;32m    453\u001b[0m nested_dirname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\n\u001b[1;32m    454\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(storage_folder, relative_filepath)\n\u001b[1;32m    455\u001b[0m )\n\u001b[1;32m    456\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(nested_dirname, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 458\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mcached_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    469\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:32\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     34\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     37\u001b[0m ]\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/huggingface_hub/file_download.py:671\u001b[0m, in \u001b[0;36mcached_download\u001b[0;34m(url, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m    669\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 671\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, cache_path)\n\u001b[1;32m    680\u001b[0m os\u001b[38;5;241m.\u001b[39mreplace(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:32\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     34\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     37\u001b[0m ]\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/huggingface_hub/file_download.py:436\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    427\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    428\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m    429\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    430\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[1;32m    435\u001b[0m )\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    438\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/requests/models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 760\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    761\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/urllib3/response.py:575\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_fp_closed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    576\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m/data/env/lib/python3.8/site-packages/urllib3/util/response.py:17\u001b[0m, in \u001b[0;36mis_fp_closed\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_fp_closed\u001b[39m(obj):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Checks whether a given file-like object is closed.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    :param obj:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        The file-like object to check.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Check `isclosed()` first, in case Python3 doesn't set `closed`.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# GH Issue #928\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39misclosed()\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bigscience/bloom')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
