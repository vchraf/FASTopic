{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363513b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "msgs = \"Ran from Airflow at 2022-07-09T04:00:00+00:00!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2ecfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import dateparser\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2a66e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID = \"005\"\n",
    "SITE = \"rewmi.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59135e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = [\"actualite\",\"faits-divers\",\"economie\",\"people\",\"revue-de-presse\",\"decryptage\",\"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79aeec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parseDate(date):\n",
    "    try:\n",
    "        return datetime.strptime(str(date), '%Y-%m-%dT%H:%M:%S+00:00')\n",
    "    except:\n",
    "        return dateparser.parse(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827f6b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleDate(HTML):\n",
    "    try:\n",
    "        return HTML.find_all(\"meta\", {\"property\": \"article:published_time\"})[0]['content']\n",
    "    except: return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c853ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getWebPage(URL):\n",
    "    page  = requests.get(URL)\n",
    "    return BeautifulSoup(page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98637248",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleNbrReader(HTML):\n",
    "    try:\n",
    "        h = HTML.find_all(\"span\", {\"class\": \"post-views\"})[0]\n",
    "        return str(h.text).split(\" \")[:-1][0]\n",
    "    except: return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021d35e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleTitle(HTML):       \n",
    "    h = HTML.find_all(\"h1\", {\"class\": \"name post-title entry-title\"})\n",
    "    return str(h[0].text).replace(\"\\n\",\"\").replace(\"\\t\", \"\").replace(\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f57e6e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleContent(HTML):\n",
    "    h = HTML.find_all(\"div\", {\"class\": \"entry\"})[0].find_all(\"p\")\n",
    "    return \" \".join([i.text for i in h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d18a8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticlesList(KEYWORD):\n",
    "    page_index = 1\n",
    "    listArt_links = []\n",
    "    while(True):\n",
    "        html = getWebPage(f\"https://www.rewmi.com/category/{KEYWORD}/page/{page_index}/\")\n",
    "        listArt = html.find_all(\"article\", {\"class\": \"item-list\"})\n",
    "        if len(listArt) == 0: break\n",
    "        heartbeat(ID)\n",
    "        listArt_links.append([i.find('a', href=True)['href'] for i in listArt])\n",
    "        if parseTime(getWebPage(listArt_links[-1][-1]).find_all(\"meta\", {\"property\": \"article:published_time\"})[0]['content']).year == 2019:break\n",
    "        page_index+=1\n",
    "    return listArt_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7281a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getLinks(category, index):\n",
    "    page = getWebPage(f\"https://www.rewmi.com/category/{category}/page/{index}/\")\n",
    "    _links = page.find_all(\"article\", {\"class\": \"item-list\"})\n",
    "    if len(_links) == 0: return []    \n",
    "    return [i.find('a', href=True)['href'] for i in _links]    \n",
    "\n",
    "\n",
    "def getNewLinks(category, checkPoint):\n",
    "    index = 1\n",
    "    links = []\n",
    "    while(True):\n",
    "        _links = getLinks(category, index)\n",
    "        if _links == []:return links\n",
    "        if checkPoint[0] in _links or checkPoint[1] in _links:\n",
    "            try:\n",
    "                lastIndex = _links.index(checkPoint[0])\n",
    "            except: lastIndex = _links.index(checkPoint[0])\n",
    "            if lastIndex == 0:\n",
    "                return []\n",
    "            else:\n",
    "                links.extend(_links)\n",
    "                return links[:lastIndex]\n",
    "        else:\n",
    "            links.extend(_links)\n",
    "            index+=1\n",
    "    return links     \n",
    "\n",
    "\n",
    "\n",
    "def getArticle(link,category):\n",
    "        try:\n",
    "            p = getWebPage(link)\n",
    "            _title      = getArticleTitle(p)\n",
    "            _article    = getArticleContent(p)\n",
    "            _date       = getArticleDate(p)\n",
    "            _author     = \"\"\n",
    "            _commments  = \"\"\n",
    "            return [1,{'DATE_SCRAPING':str(datetime.today()),\n",
    "                  'DATE':parseDate(_date),\n",
    "                  'DATE_ARTICLE':_date,\n",
    "                  'CATEGORY':category,\n",
    "                  'URL':link,\n",
    "                  \"AUTEUR\":_author,\n",
    "                  \"TITRE\":_title,\n",
    "                  \"SITE\":SITE,\n",
    "                  \"CONTENU\":_article,\n",
    "                  \"COMMENTAIRES\":_commments,}]\n",
    "        except:\n",
    "            return [0,link]\n",
    "\n",
    "\n",
    "\n",
    "def updateJson(path, key, value):\n",
    "    if not os.path.exists(path):\n",
    "        _data = {key:[]}\n",
    "    else:\n",
    "        try:\n",
    "            jsonFile = open(path, \"r\",encoding='utf-8')\n",
    "            _data = json.load(jsonFile)\n",
    "            jsonFile.close()\n",
    "        except:\n",
    "            print(\"error UpdateJson\")\n",
    "    _data[key].append(value)\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data, default=str))\n",
    "    jsonFile.close()\n",
    "    \n",
    "        \n",
    "def saveArticle(document):\n",
    "    Y = document[\"DATE\"].year\n",
    "    M = document[\"DATE\"].month\n",
    "    if not os.path.exists(f\"/data/notebooks/DB/DATA/{Y}\"):\n",
    "        os.mkdir(f\"/data/notebooks/DB/DATA/{Y}\") \n",
    "    if not os.path.exists(f\"/data/notebooks/DB/DATA/{Y}/{M}\"):\n",
    "        os.mkdir(f\"/data/notebooks/DB/DATA/{Y}/{M}\")\n",
    "    return updateJson(f\"/data/notebooks/DB/DATA/{Y}/{M}/DATA{ID}.json\", \"DATA\", document)\n",
    "\n",
    "\n",
    "def saveErrors(errList):\n",
    "    path =  f\"/data/notebooks/DB/ERRORLOGS/ERRLOGS{ID}.json\"\n",
    "    Y = datetime.today().year\n",
    "    M = datetime.today().month\n",
    "    try:\n",
    "        jsonFile = open(path, \"r\",encoding='utf-8')\n",
    "        _data = json.load(jsonFile)\n",
    "        jsonFile.close()\n",
    "    except:\n",
    "        print(\"error in saveErrors\")\n",
    "        \n",
    "    if f\"{Y}{M}\" not in _data: \n",
    "        _data[f\"{Y}{M}\"] = [errList]\n",
    "    else:\n",
    "        _data[f\"{Y}{M}\"].append(errList)\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data, default=str))\n",
    "    jsonFile.close()\n",
    "\n",
    "\n",
    "def checkPoint(category, link=None):\n",
    "    path = f\"/data/notebooks/DB/CHECKPOINT/CP{ID}.json\"\n",
    "    try:\n",
    "        jsonFile = open(path, \"r\",encoding='utf-8')\n",
    "        _data = json.load(jsonFile)\n",
    "        jsonFile.close()\n",
    "    except:\n",
    "        print(\"ERR\")\n",
    "    if link == None: return [_data[category][\"cp1\"], _data[category][\"cp2\"]]\n",
    "    _data[category][\"cp2\"] = _data[category][\"cp1\"]\n",
    "    _data[category][\"cp1\"] = link\n",
    "    _data[category][\"time\"] = str(datetime.today())\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data))\n",
    "    jsonFile.close()\n",
    "    \n",
    "\n",
    "    \n",
    "def update():\n",
    "    error=[]\n",
    "    for category in categories:\n",
    "        _checkPoint = checkPoint(category)\n",
    "        links = getNewLinks(category, _checkPoint)\n",
    "        i = 0\n",
    "        for link in links[::-1]:\n",
    "            i+=1\n",
    "            status, document = getArticle(link,category)\n",
    "            if status==0:error.append(document); continue\n",
    "            saveArticle(document)\n",
    "            checkPoint(category,link)\n",
    "    saveErrors(error)\n",
    "\n",
    "    \n",
    "    \n",
    "def initCP():\n",
    "    path = f\"/data/notebooks/DB/CHECKPOINT/CP{ID}.json\"\n",
    "    _data = {}\n",
    "    for c in categories:\n",
    "        _data[c] =\"\"\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data))\n",
    "    jsonFile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c92427",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/data/notebooks/SN/Scraper005[rewmi.com].ipynb",
   "output_path": "/data/notebooks/logs/Scraper005.ipynb",
   "parameters": {
    "msgs": "Ran from Airflow at 2022-07-09T04:00:00+00:00!"
   },
   "start_time": "2022-07-15T10:09:04.215966",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}