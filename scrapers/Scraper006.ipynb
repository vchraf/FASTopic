{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5f127f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T10:09:02.731437Z",
     "iopub.status.busy": "2022-07-15T10:09:02.715076Z",
     "iopub.status.idle": "2022-07-15T10:09:02.732658Z",
     "shell.execute_reply": "2022-07-15T10:09:02.733148Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.073612,
     "end_time": "2022-07-15T10:09:02.733341",
     "exception": false,
     "start_time": "2022-07-15T10:09:02.659729",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "msgs = \"Ran from Airflow at 2022-07-09T04:00:00+00:00!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2ecfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2022-07-15T10:09:02.788577",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import dateparser\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab243974",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID = '006'\n",
    "SITE = \"pressafrik.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194da5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = [\"Monde_r14\",\"Afrique_r22\",\"Opinion_r12\",\"Societe_r23\",\"Medias_r25\",\"Portrait_r24\",\"Reportage_r11\",\"Economie_r9\",\"Politique_r1\",\"Editorial_r13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2e89f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parseDate(date):\n",
    "    try:\n",
    "        return dateparser.parse(date, languages=['fr'])\n",
    "    except:\n",
    "        return dateparser.parse(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c853ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getWebPage(URL):\n",
    "    page  = requests.get(URL)\n",
    "    return BeautifulSoup(page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021d35e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleTitle(HTML):       \n",
    "    h = HTML.find_all(\"h1\", {\"class\": \"access\"})\n",
    "    return str(h[0].text).replace(\"\\n\",\"\").replace(\"\\t\", \"\").replace(\"\\r\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f57e6e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleContent(HTML):\n",
    "    h = HTML.find_all(\"div\", {\"class\": \"access firstletter\"})\n",
    "    return str(h[0].text).replace(\"\\n\",\"\").replace(\"\\t\", \"\").replace(\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4c32d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleAuthor(HTML):\n",
    "    try:\n",
    "        h = HTML.find_all(\"div\", {\"class\": \"real-auteur auteur\"})\n",
    "        return str(h[0].text).replace(\"\\n\",\"\").replace(\"\\t\", \"\").replace(\"\\r\", \"\")\n",
    "    except: return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b224c68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticleDate(HTML):\n",
    "    try:\n",
    "        h = HTML.find_all(\"div\", {\"id\": \"date\"})\n",
    "        return str(h[0].text).replace(\"\\n\",\"\").replace(\"\\t\", \"\").replace(\"\\r\", \"\")\n",
    "    except: return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a31d03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticlesList(KEYWORD):\n",
    "    listArt_links = []\n",
    "    URL = \"https://www.pressafrik.com\"\n",
    "    step = 5\n",
    "    index = 0\n",
    "    s = 0\n",
    "    while True:\n",
    "        heartbeat(ID)\n",
    "        html = getWebPage(f\"https://www.pressafrik.com/{KEYWORD}.html?start={index}\")\n",
    "        listArt =[URL+i.find(\"a\",href=True)['href'] for i in html.find_all(\"div\",{\"id\":\"z_col1\"})[0].find_all(\"h3\")]\n",
    "        if index>0 and listArt[-1]==listArt_links[-1][-1]:break\n",
    "        listArt_links.append(listArt)\n",
    "        if parseTime(getArticleDate(getWebPage(listArt_links[-1][-1]))).year <= 2019:\n",
    "            if s>0: return listArt_links\n",
    "            s+=1\n",
    "        index+=step\n",
    "    return listArt_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42128e18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getData(listArt_links):\n",
    "    data = []\n",
    "    err_list = []\n",
    "    for link in listArt_links:\n",
    "        try:\n",
    "            heartbeat(ID)\n",
    "            p = getWebPage(link)\n",
    "            _title      = getArticleTitle(p)\n",
    "            _article    = getArticleContent(p)\n",
    "            _date       = getArticleDate(p)\n",
    "            _author     = getArticleAuthor(p)\n",
    "            _commments  = \"\"\n",
    "            data.append({'DATE_SCRAPING':str(datetime.datetime.today()),\n",
    "                  'DATE_ARTICLE':_date,\n",
    "                  'URL':link,\n",
    "                  \"AUTEUR\":_author,\n",
    "                  \"TITRE\":_title,\n",
    "                  \"SITE\":\"pressafrik.com\",\n",
    "                  \"CONTENU\":_article,\n",
    "                  \"COMMENTAIRES\":_commments,})\n",
    "        except:\n",
    "            err_list.append(link)\n",
    "    return data, err_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8906a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getLinks(category, index):\n",
    "    URL = \"https://www.pressafrik.com\"\n",
    "    page = getWebPage(f\"https://www.pressafrik.com/{category}.html?start={index}\")\n",
    "    _links =[URL+i.find(\"a\",href=True)['href'] for i in page.find_all(\"div\",{\"id\":\"z_col1\"})[0].find_all(\"h3\")]\n",
    "    if len(_links)==0: return []\n",
    "    return _links \n",
    "\n",
    "\n",
    "def getNewLinks(category, checkPoint):\n",
    "    links = []\n",
    "    index  = 0\n",
    "    step = 5\n",
    "    while(True):\n",
    "        _links = getLinks(category, index)\n",
    "        if _links == []:return links\n",
    "        if checkPoint[0] in _links or checkPoint[1] in _links:\n",
    "            try:\n",
    "                lastIndex = _links.index(checkPoint[0])\n",
    "            except: lastIndex = _links.index(checkPoint[0])\n",
    "            if lastIndex == 0:\n",
    "                return []\n",
    "            else:\n",
    "                links.extend(_links)\n",
    "                return links[:lastIndex]\n",
    "        else:\n",
    "            links.extend(_links)\n",
    "            index+=step\n",
    "    return links     \n",
    "\n",
    "\n",
    "def getArticle(link,category):\n",
    "        try:\n",
    "            p = getWebPage(link)\n",
    "            _title      = getArticleTitle(p)\n",
    "            _article    = getArticleContent(p)\n",
    "            _date       = getArticleDate(p)\n",
    "            _author     = getArticleAuthor(p)\n",
    "            _commments  = \"\"\n",
    "            return [1,{'DATE_SCRAPING':str(datetime.today()),\n",
    "                  'DATE':parseDate(_date),\n",
    "                  'DATE_ARTICLE':_date,\n",
    "                  'CATEGORY':category,\n",
    "                  'URL':link,\n",
    "                  \"AUTEUR\":_author,\n",
    "                  \"TITRE\":_title,\n",
    "                  \"SITE\":SITE,\n",
    "                  \"CONTENU\":_article,\n",
    "                  \"COMMENTAIRES\":_commments,}]\n",
    "        except:\n",
    "            return [0,link]\n",
    "\n",
    "\n",
    "\n",
    "def updateJson(path, key, value):\n",
    "    if not os.path.exists(path):\n",
    "        _data = {key:[]}\n",
    "    else:\n",
    "        try:\n",
    "            jsonFile = open(path, \"r\",encoding='utf-8')\n",
    "            _data = json.load(jsonFile)\n",
    "            jsonFile.close()\n",
    "        except:\n",
    "            print(\"error UpdateJson\")\n",
    "    _data[key].append(value)\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data, default=str))\n",
    "    jsonFile.close()\n",
    "    \n",
    "        \n",
    "def saveArticle(document):\n",
    "    Y = document[\"DATE\"].year\n",
    "    M = document[\"DATE\"].month\n",
    "    if not os.path.exists(f\"/data/notebooks/DB/DATA/{Y}\"):\n",
    "        os.mkdir(f\"/data/notebooks/DB/DATA/{Y}\") \n",
    "    if not os.path.exists(f\"/data/notebooks/DB/DATA/{Y}/{M}\"):\n",
    "        os.mkdir(f\"/data/notebooks/DB/DATA/{Y}/{M}\")\n",
    "    return updateJson(f\"/data/notebooks/DB/DATA/{Y}/{M}/DATA{ID}.json\", \"DATA\", document)\n",
    "\n",
    "\n",
    "def saveErrors(errList):\n",
    "    path =  f\"/data/notebooks/DB/ERRORLOGS/ERRLOGS{ID}.json\"\n",
    "    Y = datetime.today().year\n",
    "    M = datetime.today().month\n",
    "    try:\n",
    "        jsonFile = open(path, \"r\",encoding='utf-8')\n",
    "        _data = json.load(jsonFile)\n",
    "        jsonFile.close()\n",
    "    except:\n",
    "        print(\"error in saveErrors\")\n",
    "        \n",
    "    if f\"{Y}{M}\" not in _data: \n",
    "        _data[f\"{Y}{M}\"] = [errList]\n",
    "    else:\n",
    "        _data[f\"{Y}{M}\"].append(errList)\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data, default=str))\n",
    "    jsonFile.close()\n",
    "\n",
    "\n",
    "\n",
    "def checkPoint(category, link=None):\n",
    "    path = f\"/data/notebooks/DB/CHECKPOINT/CP{ID}.json\"\n",
    "    try:\n",
    "        jsonFile = open(path, \"r\",encoding='utf-8')\n",
    "        _data = json.load(jsonFile)\n",
    "        jsonFile.close()\n",
    "    except:\n",
    "        print(\"ERR\")\n",
    "    if link == None: return [_data[category][\"cp1\"], _data[category][\"cp2\"]]\n",
    "    _data[category][\"cp2\"] = _data[category][\"cp1\"]\n",
    "    _data[category][\"cp1\"] = link\n",
    "    _data[category][\"time\"] = str(datetime.today())\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data))\n",
    "    jsonFile.close()\n",
    "    \n",
    "\n",
    "    \n",
    "def update():\n",
    "    error=[]\n",
    "    for category in categories:\n",
    "        _checkPoint = checkPoint(category)\n",
    "        links = getNewLinks(category, _checkPoint)\n",
    "        i = 0\n",
    "        for link in links[::-1]:\n",
    "            i+=1\n",
    "            status, document = getArticle(link,category)\n",
    "            if status==0:error.append(document); continue\n",
    "            saveArticle(document)\n",
    "            checkPoint(category,link)\n",
    "    saveErrors(error)\n",
    "\n",
    "\n",
    "def initCP():\n",
    "    path = f\"/data/notebooks/DB/CHECKPOINT/CP{ID}.json\"\n",
    "    _data = {}\n",
    "    for c in categories:\n",
    "        _data[c] =\"\"\n",
    "    jsonFile = open(path, \"w+\", encoding ='utf-8')\n",
    "    jsonFile.write(json.dumps(_data))\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cb6b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/data/notebooks/SN/Scraper006[pressafrik.com].ipynb",
   "output_path": "/data/notebooks/logs/Scraper006.ipynb",
   "parameters": {
    "msgs": "Ran from Airflow at 2022-07-09T04:00:00+00:00!"
   },
   "start_time": "2022-07-15T10:08:59.018916",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}